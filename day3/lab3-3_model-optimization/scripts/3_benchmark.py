#!/usr/bin/env python3
"""
Lab 3-3 Part 3: Benchmark (ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬)
ì›ë³¸, ONNX, ì–‘ìí™” ëª¨ë¸ì˜ ì¶”ë¡  ì„±ëŠ¥ì„ ë¹„êµí•©ë‹ˆë‹¤.

ì‹¤í–‰ ë°©ë²•:
    python scripts/3_benchmark.py

ì‚¬ì „ ìš”êµ¬ì‚¬í•­:
    - Part 1, 2 ì‹¤í–‰ ì™„ë£Œ
    - outputs/model_original.joblib
    - outputs/model_optimized.onnx
    - outputs/model_quantized.onnx

í™˜ê²½ë³€ìˆ˜ (ì„ íƒ):
    - MLFLOW_TRACKING_URI: MLflow ì„œë²„ ì£¼ì†Œ (ì˜ˆ: http://mlflow-server-service.mlflow-system.svc.cluster.local:5000)
"""

import os
import sys
import time
import numpy as np
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì„¤ì •
PROJECT_ROOT = Path(__file__).parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

# í•„ìš”í•œ íŒ¨í‚¤ì§€ ì„í¬íŠ¸
try:
    import joblib
    import onnxruntime as ort
    from sklearn.datasets import load_iris
    from sklearn.model_selection import train_test_split
    from sklearn.metrics import accuracy_score
except ImportError as e:
    print(f"âŒ í•„ìš”í•œ íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤: {e}")
    print("   pip install joblib onnxruntime scikit-learn ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
    sys.exit(1)

# MLflow (ì„ íƒì )
try:
    import mlflow
    MLFLOW_AVAILABLE = True
except ImportError:
    MLFLOW_AVAILABLE = False


def print_header(title: str):
    """ì„¹ì…˜ í—¤ë” ì¶œë ¥"""
    print("\n" + "=" * 60)
    print(f"  {title}")
    print("=" * 60)


def print_step(step: int, description: str):
    """ë‹¨ê³„ ì¶œë ¥"""
    print(f"\nStep {step}: {description}")
    print("â”€" * 40)


def get_file_size(filepath: str) -> float:
    """íŒŒì¼ í¬ê¸°ë¥¼ KB ë‹¨ìœ„ë¡œ ë°˜í™˜"""
    return os.path.getsize(filepath) / 1024


def benchmark_inference(predict_fn, X_test: np.ndarray, n_iterations: int = 1000) -> dict:
    """ì¶”ë¡  ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬"""
    # ì›Œë°ì—…
    for _ in range(10):
        predict_fn(X_test)
    
    # ë²¤ì¹˜ë§ˆí¬
    times = []
    for _ in range(n_iterations):
        start = time.perf_counter()
        predict_fn(X_test)
        end = time.perf_counter()
        times.append((end - start) * 1000)  # msë¡œ ë³€í™˜
    
    return {
        "mean": np.mean(times),
        "std": np.std(times),
        "min": np.min(times),
        "max": np.max(times),
        "median": np.median(times),
    }


def get_mlflow_tracking_uri():
    """
    MLflow Tracking URIë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
    í™˜ê²½ë³€ìˆ˜ > ê¸°ë³¸ê°’ ìˆœì„œë¡œ í™•ì¸í•©ë‹ˆë‹¤.
    """
    # í™˜ê²½ë³€ìˆ˜ì—ì„œ ê°€ì ¸ì˜¤ê¸°
    tracking_uri = os.environ.get("MLFLOW_TRACKING_URI")
    
    if tracking_uri:
        return tracking_uri
    
    # Kubeflow/Kubernetes í™˜ê²½ì—ì„œ ì¼ë°˜ì ì¸ MLflow ì„œë¹„ìŠ¤ ì£¼ì†Œ
    default_uri = "http://mlflow-server-service.mlflow-system.svc.cluster.local:5000"
    
    return default_uri


def main():
    print_header("Lab 3-3 Part 3: Benchmark")
    
    # ê²½ë¡œ ì„¤ì • (outputs í´ë” ì‚¬ìš©)
    outputs_dir = PROJECT_ROOT / "outputs"
    original_model_path = outputs_dir / "model_original.joblib"
    onnx_model_path = outputs_dir / "model_optimized.onnx"
    quantized_model_path = outputs_dir / "model_quantized.onnx"
    
    # íŒŒì¼ ì¡´ì¬ í™•ì¸
    missing_files = []
    if not original_model_path.exists():
        missing_files.append(str(original_model_path))
    if not onnx_model_path.exists():
        missing_files.append(str(onnx_model_path))
    if not quantized_model_path.exists():
        missing_files.append(str(quantized_model_path))
    
    if missing_files:
        print("âŒ ë‹¤ìŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤:")
        for f in missing_files:
            print(f"   - {f}")
        print("\n   Part 1, 2ë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.")
        sys.exit(1)
    
    # =========================================================
    # Step 1: í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¤€ë¹„
    # =========================================================
    print_step(1, "í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¤€ë¹„")
    
    iris = load_iris()
    X, y = iris.data, iris.target
    _, X_test, _, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    X_test_float32 = X_test.astype(np.float32)
    
    print(f"  ğŸ“Š í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ ìˆ˜: {len(X_test)}")
    print(f"  ğŸ“Š í”¼ì²˜ ìˆ˜: {X_test.shape[1]}")
    
    # =========================================================
    # Step 2: ëª¨ë¸ ë¡œë“œ
    # =========================================================
    print_step(2, "ëª¨ë¸ ë¡œë“œ")
    
    # ì›ë³¸ sklearn ëª¨ë¸
    print("  ğŸ“¦ ì›ë³¸ sklearn ëª¨ë¸ ë¡œë“œ...")
    original_model = joblib.load(original_model_path)
    
    # ONNX ëª¨ë¸
    print("  ğŸ“¦ ONNX ëª¨ë¸ ë¡œë“œ...")
    onnx_session = ort.InferenceSession(str(onnx_model_path))
    onnx_input_name = onnx_session.get_inputs()[0].name
    
    # ì–‘ìí™” ëª¨ë¸
    print("  ğŸ“¦ ì–‘ìí™” ëª¨ë¸ ë¡œë“œ...")
    quant_session = ort.InferenceSession(str(quantized_model_path))
    quant_input_name = quant_session.get_inputs()[0].name
    
    print("  âœ… ëª¨ë“  ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
    
    # =========================================================
    # Step 3: ì •í™•ë„ ë¹„êµ
    # =========================================================
    print_step(3, "ì •í™•ë„ ë¹„êµ")
    
    # ì›ë³¸ ëª¨ë¸
    original_pred = original_model.predict(X_test)
    original_accuracy = accuracy_score(y_test, original_pred)
    
    # ONNX ëª¨ë¸
    onnx_output = onnx_session.run(None, {onnx_input_name: X_test_float32})
    onnx_pred = onnx_output[0]
    if len(onnx_pred.shape) > 1 and onnx_pred.shape[1] > 1:
        onnx_pred = np.argmax(onnx_pred, axis=1)
    onnx_accuracy = accuracy_score(y_test, onnx_pred)
    
    # ì–‘ìí™” ëª¨ë¸
    quant_output = quant_session.run(None, {quant_input_name: X_test_float32})
    quant_pred = quant_output[0]
    if len(quant_pred.shape) > 1 and quant_pred.shape[1] > 1:
        quant_pred = np.argmax(quant_pred, axis=1)
    quant_accuracy = accuracy_score(y_test, quant_pred)
    
    print(f"  ğŸ“ˆ ì›ë³¸ ëª¨ë¸ ì •í™•ë„:   {original_accuracy:.4f} ({original_accuracy*100:.2f}%)")
    print(f"  ğŸ“ˆ ONNX ëª¨ë¸ ì •í™•ë„:   {onnx_accuracy:.4f} ({onnx_accuracy*100:.2f}%)")
    print(f"  ğŸ“ˆ ì–‘ìí™” ëª¨ë¸ ì •í™•ë„: {quant_accuracy:.4f} ({quant_accuracy*100:.2f}%)")
    
    # =========================================================
    # Step 4: ì¶”ë¡  ì†ë„ ë²¤ì¹˜ë§ˆí¬
    # =========================================================
    print_step(4, "ì¶”ë¡  ì†ë„ ë²¤ì¹˜ë§ˆí¬")
    
    n_iterations = 1000
    print(f"  â±ï¸ ê° ëª¨ë¸ {n_iterations}íšŒ ì¶”ë¡  ìˆ˜í–‰ ì¤‘...")
    
    # ì›ë³¸ ëª¨ë¸ ë²¤ì¹˜ë§ˆí¬
    print("  ğŸ”„ ì›ë³¸ sklearn ëª¨ë¸ ë²¤ì¹˜ë§ˆí¬...")
    original_benchmark = benchmark_inference(
        lambda x: original_model.predict(x),
        X_test,
        n_iterations
    )
    
    # ONNX ëª¨ë¸ ë²¤ì¹˜ë§ˆí¬
    print("  ğŸ”„ ONNX ëª¨ë¸ ë²¤ì¹˜ë§ˆí¬...")
    onnx_benchmark = benchmark_inference(
        lambda x: onnx_session.run(None, {onnx_input_name: x.astype(np.float32)})[0],
        X_test,
        n_iterations
    )
    
    # ì–‘ìí™” ëª¨ë¸ ë²¤ì¹˜ë§ˆí¬
    print("  ğŸ”„ ì–‘ìí™” ëª¨ë¸ ë²¤ì¹˜ë§ˆí¬...")
    quant_benchmark = benchmark_inference(
        lambda x: quant_session.run(None, {quant_input_name: x.astype(np.float32)})[0],
        X_test,
        n_iterations
    )
    
    # ì†ë„ í–¥ìƒ ê³„ì‚°
    onnx_speedup = original_benchmark["mean"] / onnx_benchmark["mean"]
    quant_speedup = original_benchmark["mean"] / quant_benchmark["mean"]
    
    print(f"\n  â±ï¸ ì¶”ë¡  ì‹œê°„ (í‰ê· , {n_iterations}íšŒ):")
    print(f"     â€¢ ì›ë³¸ sklearn: {original_benchmark['mean']:.4f} ms (1.0x)")
    print(f"     â€¢ ONNX:         {onnx_benchmark['mean']:.4f} ms ({onnx_speedup:.1f}x)")
    print(f"     â€¢ ì–‘ìí™”:       {quant_benchmark['mean']:.4f} ms ({quant_speedup:.1f}x)")
    
    # =========================================================
    # Step 5: ëª¨ë¸ í¬ê¸° ë¹„êµ
    # =========================================================
    print_step(5, "ëª¨ë¸ í¬ê¸° ë¹„êµ")
    
    original_size = get_file_size(original_model_path)
    onnx_size = get_file_size(onnx_model_path)
    quant_size = get_file_size(quantized_model_path)
    
    onnx_reduction = (1 - onnx_size / original_size) * 100
    quant_reduction = (1 - quant_size / original_size) * 100
    
    print(f"  ğŸ“¦ ì›ë³¸ sklearn: {original_size:.2f} KB (100%)")
    print(f"  ğŸ“¦ ONNX:         {onnx_size:.2f} KB ({100-onnx_reduction:.1f}%, -{onnx_reduction:.1f}%)")
    print(f"  ğŸ“¦ ì–‘ìí™”:       {quant_size:.2f} KB ({100-quant_reduction:.1f}%, -{quant_reduction:.1f}%)")
    
    # =========================================================
    # Step 6: MLflow ê¸°ë¡
    # =========================================================
    print_step(6, "MLflow ê¸°ë¡")
    
    if MLFLOW_AVAILABLE:
        try:
            # â­ MLflow Tracking URI ì„¤ì •
            tracking_uri = get_mlflow_tracking_uri()
            print(f"  ğŸ”— MLflow Tracking URI: {tracking_uri}")
            mlflow.set_tracking_uri(tracking_uri)
            
            # ì‹¤í—˜ ì„¤ì • (Lab 3-3ìœ¼ë¡œ ë³€ê²½)
            experiment_name = "lab3-3-model-optimization"
            
            # ì‹¤í—˜ì´ ì—†ìœ¼ë©´ ìƒì„±
            experiment = mlflow.get_experiment_by_name(experiment_name)
            if experiment is None:
                experiment_id = mlflow.create_experiment(experiment_name)
                print(f"  ğŸ“ ìƒˆ ì‹¤í—˜ ìƒì„±: {experiment_name}")
            else:
                experiment_id = experiment.experiment_id
                print(f"  ğŸ“ ê¸°ì¡´ ì‹¤í—˜ ì‚¬ìš©: {experiment_name}")
            
            mlflow.set_experiment(experiment_name)
            
            with mlflow.start_run(run_name="benchmark-results"):
                # í¬ê¸° ë©”íŠ¸ë¦­
                mlflow.log_metric("original_size_kb", original_size)
                mlflow.log_metric("onnx_size_kb", onnx_size)
                mlflow.log_metric("quantized_size_kb", quant_size)
                mlflow.log_metric("size_reduction_percent", quant_reduction)
                
                # ì†ë„ ë©”íŠ¸ë¦­
                mlflow.log_metric("original_inference_ms", original_benchmark["mean"])
                mlflow.log_metric("onnx_inference_ms", onnx_benchmark["mean"])
                mlflow.log_metric("quantized_inference_ms", quant_benchmark["mean"])
                mlflow.log_metric("onnx_speedup", onnx_speedup)
                mlflow.log_metric("quantized_speedup", quant_speedup)
                
                # ì •í™•ë„ ë©”íŠ¸ë¦­
                mlflow.log_metric("original_accuracy", original_accuracy)
                mlflow.log_metric("onnx_accuracy", onnx_accuracy)
                mlflow.log_metric("quantized_accuracy", quant_accuracy)
                
                # íŒŒë¼ë¯¸í„°
                mlflow.log_param("n_iterations", n_iterations)
                mlflow.log_param("test_samples", len(X_test))
                mlflow.log_param("quantization_type", "dynamic_uint8")
                
                # ëª¨ë¸ ì•„í‹°íŒ©íŠ¸
                mlflow.log_artifact(str(onnx_model_path))
                mlflow.log_artifact(str(quantized_model_path))
                
                run_id = mlflow.active_run().info.run_id
                print("  âœ… MLflowì— ê²°ê³¼ ê¸°ë¡ ì™„ë£Œ")
                print(f"     Run ID: {run_id}")
                print(f"     ì‹¤í—˜: {experiment_name}")
                
        except Exception as e:
            print(f"  âš ï¸ MLflow ê¸°ë¡ ì‹¤íŒ¨: {e}")
            print("     (MLflow ì„œë²„ ì—°ê²°ì„ í™•ì¸í•˜ì„¸ìš”)")
            print("\n  ğŸ’¡ í•´ê²° ë°©ë²•:")
            print("     1. í™˜ê²½ë³€ìˆ˜ ì„¤ì •: export MLFLOW_TRACKING_URI=http://mlflow-server-service.mlflow-system.svc.cluster.local:5000")
            print("     2. ë˜ëŠ” Kubeflow Jupyter í™˜ê²½ì—ì„œ ì‹¤í–‰í•˜ì„¸ìš”")
    else:
        print("  âš ï¸ MLflowê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•„ ê¸°ë¡ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
        print("     (pip install mlflowë¡œ ì„¤ì¹˜ ê°€ëŠ¥)")
    
    # =========================================================
    # Step 7: ìµœì¢… ê²°ê³¼ ìš”ì•½
    # =========================================================
    print_step(7, "ìµœì¢… ê²°ê³¼ ìš”ì•½")
    
    print("""
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚                      ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ìš”ì•½                          â”‚
  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
  â”‚  ëª¨ë¸          â”‚ í¬ê¸° (KB) â”‚ ì¶”ë¡  (ms) â”‚ ì†ë„í–¥ìƒ â”‚ ì •í™•ë„    â”‚
  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤""")
    print(f"  â”‚  ì›ë³¸ sklearn â”‚ {original_size:>9.2f} â”‚ {original_benchmark['mean']:>9.4f} â”‚ {1.0:>8.1f}x â”‚ {original_accuracy*100:>8.2f}% â”‚")
    print(f"  â”‚  ONNX         â”‚ {onnx_size:>9.2f} â”‚ {onnx_benchmark['mean']:>9.4f} â”‚ {onnx_speedup:>8.1f}x â”‚ {onnx_accuracy*100:>8.2f}% â”‚")
    print(f"  â”‚  ì–‘ìí™”       â”‚ {quant_size:>9.2f} â”‚ {quant_benchmark['mean']:>9.4f} â”‚ {quant_speedup:>8.1f}x â”‚ {quant_accuracy*100:>8.2f}% â”‚")
    print("""  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    """)
    
    print("  ğŸ¯ í•µì‹¬ ì„±ê³¼:")
    print(f"     â€¢ ëª¨ë¸ í¬ê¸°: {original_size:.1f} KB â†’ {quant_size:.1f} KB ({quant_reduction:.1f}% ê°ì†Œ)")
    print(f"     â€¢ ì¶”ë¡  ì†ë„: {original_benchmark['mean']:.2f} ms â†’ {quant_benchmark['mean']:.2f} ms ({quant_speedup:.1f}x í–¥ìƒ)")
    print(f"     â€¢ ì •í™•ë„ ìœ ì§€: {quant_accuracy*100:.2f}% (ì†ì‹¤ ì—†ìŒ)")
    
    print("\n" + "=" * 60)
    print("  âœ… Lab 3-3 ì™„ë£Œ! ëª¨ë¸ ìµœì í™” ì‹¤ìŠµì„ ì„±ê³µì ìœ¼ë¡œ ë§ˆì³¤ìŠµë‹ˆë‹¤.")
    print("=" * 60 + "\n")


if __name__ == "__main__":
    main()
