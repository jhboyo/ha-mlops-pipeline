{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ¯ Day 3 ì¡°ë³„ í”„ë¡œì íŠ¸: ì†”ë£¨ì…˜\n",
    "\n",
    "## âš ï¸ ì´ íŒŒì¼ì€ ë°œí‘œ í›„ì— ê³µê°œë©ë‹ˆë‹¤.\n",
    "\n",
    "ì´ ë…¸íŠ¸ë¶ì€ ì™„ì „í•œ E2E ML íŒŒì´í”„ë¼ì¸ì˜ ì†”ë£¨ì…˜ ì˜ˆì œì…ë‹ˆë‹¤.\n",
    "\n",
    "### ì†”ë£¨ì…˜ íŠ¹ì§•\n",
    "- **7ê°œ íŒŒìƒ í”¼ì²˜** êµ¬í˜„\n",
    "- **ì™„ì „í•œ MLflow í†µí•©** (íŒŒë¼ë¯¸í„°, ë©”íŠ¸ë¦­, ëª¨ë¸)\n",
    "- **í”¼ì²˜ ì¤‘ìš”ë„ ë¡œê¹…**\n",
    "- **KServe ë°°í¬** (ìƒíƒœ í™•ì¸ í¬í•¨)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. í™˜ê²½ ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# íŒ¨í‚¤ì§€ ì„¤ì¹˜\n",
    "!pip install kfp==2.7.0 mlflow>=2.9.0 scikit-learn pandas numpy kubernetes -q\n",
    "print(\"âœ… íŒ¨í‚¤ì§€ ì„¤ì¹˜ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import\n",
    "import os\n",
    "from kfp import dsl\n",
    "from kfp.dsl import component, Input, Output, Dataset, Model, Metrics\n",
    "from kfp import compiler\n",
    "import kfp\n",
    "\n",
    "print(f\"KFP Version: {kfp.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì†”ë£¨ì…˜ íŒ€ ì„¤ì •\n",
    "TEAM_NAME = \"solution-team\"\n",
    "\n",
    "# âš ï¸ ë³¸ì¸ì˜ ì‚¬ìš©ì ë²ˆí˜¸ë¡œ ë³€ê²½!\n",
    "USER_NUM = \"YOUR_USER_NUM\"\n",
    "\n",
    "USER_NAMESPACE = f\"kubeflow-user{USER_NUM}\"\n",
    "MLFLOW_TRACKING_URI = \"http://mlflow-server-service.mlflow-system.svc.cluster.local:5000\"\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"  ì†”ë£¨ì…˜ ì„¤ì •\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"  Team: {TEAM_NAME}\")\n",
    "print(f\"  Namespace: {USER_NAMESPACE}\")\n",
    "print(f\"  MLflow URI: {MLFLOW_TRACKING_URI}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. ì»´í¬ë„ŒíŠ¸ ì •ì˜ (ì™„ì„± ë²„ì „)\n",
    "\n",
    "### 2.1 Component 1: ë°ì´í„° ë¡œë“œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=\"python:3.9-slim\",\n",
    "    packages_to_install=[\"pandas==2.0.3\", \"scikit-learn==1.3.2\"]\n",
    ")\n",
    "def load_data(\n",
    "    data_source: str,\n",
    "    output_data: Output[Dataset]\n",
    "):\n",
    "    \"\"\"California Housing ë°ì´í„°ì…‹ ë¡œë“œ (ì™„ì„± ë²„ì „)\"\"\"\n",
    "    import pandas as pd\n",
    "    from sklearn.datasets import fetch_california_housing\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"  Step 1: Load Data\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    housing = fetch_california_housing(as_frame=True)\n",
    "    df = housing.frame\n",
    "    \n",
    "    print(f\"  Source: {data_source}\")\n",
    "    print(f\"  Shape: {df.shape}\")\n",
    "    print(f\"  Columns: {list(df.columns)}\")\n",
    "    print(f\"  Target statistics:\")\n",
    "    print(f\"    Mean: {df['MedHouseVal'].mean():.4f}\")\n",
    "    print(f\"    Std: {df['MedHouseVal'].std():.4f}\")\n",
    "    print(f\"    Min: {df['MedHouseVal'].min():.4f}\")\n",
    "    print(f\"    Max: {df['MedHouseVal'].max():.4f}\")\n",
    "    \n",
    "    df.to_csv(output_data.path, index=False)\n",
    "    print(f\"  âœ… Data saved\")\n",
    "\n",
    "print(\"âœ… load_data ì»´í¬ë„ŒíŠ¸ ì •ì˜ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Component 2: ì „ì²˜ë¦¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=\"python:3.9-slim\",\n",
    "    packages_to_install=[\"pandas==2.0.3\", \"scikit-learn==1.3.2\", \"numpy==1.24.3\"]\n",
    ")\n",
    "def preprocess(\n",
    "    input_data: Input[Dataset],\n",
    "    X_train_out: Output[Dataset],\n",
    "    X_test_out: Output[Dataset],\n",
    "    y_train_out: Output[Dataset],\n",
    "    y_test_out: Output[Dataset],\n",
    "    test_size: float = 0.2\n",
    "):\n",
    "    \"\"\"ë°ì´í„° ì „ì²˜ë¦¬ (ì™„ì„± ë²„ì „)\"\"\"\n",
    "    import pandas as pd\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"  Step 2: Preprocess\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    df = pd.read_csv(input_data.path)\n",
    "    print(f\"  Loaded {len(df)} rows\")\n",
    "    \n",
    "    # ê²°ì¸¡ì¹˜ í™•ì¸\n",
    "    null_count = df.isnull().sum().sum()\n",
    "    print(f\"  Null values: {null_count}\")\n",
    "    \n",
    "    X = df.drop(columns=['MedHouseVal'])\n",
    "    y = df['MedHouseVal']\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=42\n",
    "    )\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = pd.DataFrame(\n",
    "        scaler.fit_transform(X_train),\n",
    "        columns=X_train.columns\n",
    "    )\n",
    "    X_test_scaled = pd.DataFrame(\n",
    "        scaler.transform(X_test),\n",
    "        columns=X_test.columns\n",
    "    )\n",
    "    \n",
    "    X_train_scaled.to_csv(X_train_out.path, index=False)\n",
    "    X_test_scaled.to_csv(X_test_out.path, index=False)\n",
    "    y_train.to_csv(y_train_out.path, index=False)\n",
    "    y_test.to_csv(y_test_out.path, index=False)\n",
    "    \n",
    "    print(f\"  Train: {len(X_train)} samples\")\n",
    "    print(f\"  Test: {len(X_test)} samples\")\n",
    "    print(f\"  Features: {len(X_train.columns)}\")\n",
    "    print(f\"  âœ… Preprocessing completed\")\n",
    "\n",
    "print(\"âœ… preprocess ì»´í¬ë„ŒíŠ¸ ì •ì˜ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Component 3: í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ (7ê°œ í”¼ì²˜ - ì™„ì„± ë²„ì „)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=\"python:3.9-slim\",\n",
    "    packages_to_install=[\"pandas==2.0.3\", \"numpy==1.24.3\"]\n",
    ")\n",
    "def feature_engineering(\n",
    "    X_train_in: Input[Dataset],\n",
    "    X_test_in: Input[Dataset],\n",
    "    X_train_out: Output[Dataset],\n",
    "    X_test_out: Output[Dataset]\n",
    ") -> int:\n",
    "    \"\"\"\n",
    "    í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ - ì™„ì„± ë²„ì „ (7ê°œ íŒŒìƒ í”¼ì²˜)\n",
    "    \n",
    "    ìƒì„±ë˜ëŠ” í”¼ì²˜:\n",
    "    1. rooms_per_household: ê°€êµ¬ë‹¹ ë°© ìˆ˜\n",
    "    2. bedrooms_ratio: ë°© ëŒ€ë¹„ ì¹¨ì‹¤ ë¹„ìœ¨\n",
    "    3. population_per_household: ê°€êµ¬ë‹¹ ì¸êµ¬\n",
    "    4. dist_to_bay: Bay Areaê¹Œì§€ ê±°ë¦¬\n",
    "    5. density: ë°€ì§‘ë„ ì§€í‘œ\n",
    "    6. income_rooms: ì†Œë“ Ã— ë°© ìˆ˜ ìƒí˜¸ì‘ìš©\n",
    "    7. location_score: ìœ„ì¹˜ ì ìˆ˜\n",
    "    \n",
    "    Returns:\n",
    "        int: ìƒì„±ëœ í”¼ì²˜ ìˆ˜\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"  Step 3: Feature Engineering (Solution)\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    X_train = pd.read_csv(X_train_in.path)\n",
    "    X_test = pd.read_csv(X_test_in.path)\n",
    "    \n",
    "    original_cols = list(X_train.columns)\n",
    "    print(f\"  Original features: {len(original_cols)}\")\n",
    "    \n",
    "    def add_features(df):\n",
    "        \"\"\"íŒŒìƒ ë³€ìˆ˜ ì¶”ê°€ - ì™„ì„± ë²„ì „ (7ê°œ í”¼ì²˜)\"\"\"\n",
    "        df = df.copy()\n",
    "        \n",
    "        # 1. ê°€êµ¬ë‹¹ ë°© ìˆ˜\n",
    "        df['rooms_per_household'] = df['AveRooms'] / (df['AveOccup'] + 1e-6)\n",
    "        \n",
    "        # 2. ë°© ëŒ€ë¹„ ì¹¨ì‹¤ ë¹„ìœ¨\n",
    "        df['bedrooms_ratio'] = df['AveBedrms'] / (df['AveRooms'] + 1e-6)\n",
    "        \n",
    "        # 3. ê°€êµ¬ë‹¹ ì¸êµ¬\n",
    "        df['population_per_household'] = df['Population'] / (df['AveOccup'] + 1e-6)\n",
    "        \n",
    "        # 4. Bay Areaê¹Œì§€ ê±°ë¦¬ (ì •ê·œí™”ëœ ì¢Œí‘œ ê¸°ë°˜)\n",
    "        df['dist_to_bay'] = np.sqrt(\n",
    "            df['Latitude']**2 + df['Longitude']**2\n",
    "        )\n",
    "        \n",
    "        # 5. ë°€ì§‘ë„ ì§€í‘œ\n",
    "        df['density'] = df['Population'] * df['AveOccup']\n",
    "        \n",
    "        # 6. ì†Œë“ê³¼ ë°© ìˆ˜ì˜ ìƒí˜¸ì‘ìš©\n",
    "        df['income_rooms'] = df['MedInc'] * df['AveRooms']\n",
    "        \n",
    "        # 7. ìœ„ì¹˜ ì ìˆ˜ (ìœ„ë„ì™€ ê²½ë„ì˜ ì¡°í•©)\n",
    "        df['location_score'] = df['Latitude'] * 0.5 + df['Longitude'] * 0.5\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    X_train_fe = add_features(X_train)\n",
    "    X_test_fe = add_features(X_test)\n",
    "    \n",
    "    new_cols = [c for c in X_train_fe.columns if c not in original_cols]\n",
    "    \n",
    "    print(f\"  New features ({len(new_cols)}):\")\n",
    "    for feat in new_cols:\n",
    "        stats = X_train_fe[feat].describe()\n",
    "        print(f\"    - {feat}: mean={stats['mean']:.4f}, std={stats['std']:.4f}\")\n",
    "    \n",
    "    print(f\"  Total features: {len(X_train_fe.columns)}\")\n",
    "    \n",
    "    X_train_fe.to_csv(X_train_out.path, index=False)\n",
    "    X_test_fe.to_csv(X_test_out.path, index=False)\n",
    "    \n",
    "    print(f\"  âœ… Feature engineering completed\")\n",
    "    \n",
    "    return len(new_cols)\n",
    "\n",
    "print(\"âœ… feature_engineering ì»´í¬ë„ŒíŠ¸ ì •ì˜ ì™„ë£Œ! (7ê°œ í”¼ì²˜)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Component 4: ëª¨ë¸ í•™ìŠµ (ì™„ì„± ë²„ì „)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=\"python:3.9-slim\",\n",
    "    packages_to_install=[\n",
    "        \"pandas==2.0.3\",\n",
    "        \"scikit-learn==1.3.2\",\n",
    "        \"mlflow==2.9.2\",\n",
    "        \"numpy==1.24.3\",\n",
    "        \"boto3\"\n",
    "    ]\n",
    ")\n",
    "def train_model(\n",
    "    X_train: Input[Dataset],\n",
    "    X_test: Input[Dataset],\n",
    "    y_train: Input[Dataset],\n",
    "    y_test: Input[Dataset],\n",
    "    mlflow_tracking_uri: str,\n",
    "    experiment_name: str,\n",
    "    team_name: str,\n",
    "    n_estimators: int = 100,\n",
    "    max_depth: int = 10\n",
    ") -> str:\n",
    "    \"\"\"ëª¨ë¸ í•™ìŠµ ë° MLflow ê¸°ë¡ (ì™„ì„± ë²„ì „)\"\"\"\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import mlflow\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "    import os\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(f\"  Step 4: Train Model - {team_name}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    X_train_df = pd.read_csv(X_train.path)\n",
    "    X_test_df = pd.read_csv(X_test.path)\n",
    "    y_train_df = pd.read_csv(y_train.path)\n",
    "    y_test_df = pd.read_csv(y_test.path)\n",
    "    \n",
    "    print(f\"  Training data: {X_train_df.shape}\")\n",
    "    print(f\"  Test data: {X_test_df.shape}\")\n",
    "    \n",
    "    os.environ['MLFLOW_TRACKING_URI'] = mlflow_tracking_uri\n",
    "    mlflow.set_tracking_uri(mlflow_tracking_uri)\n",
    "    mlflow.set_experiment(experiment_name)\n",
    "    \n",
    "    with mlflow.start_run(run_name=f\"{team_name}-run\") as run:\n",
    "        run_id = run.info.run_id\n",
    "        print(f\"  MLflow Run ID: {run_id}\")\n",
    "        \n",
    "        # íŒŒë¼ë¯¸í„° ë¡œê¹…\n",
    "        mlflow.log_params({\n",
    "            \"n_estimators\": n_estimators,\n",
    "            \"max_depth\": max_depth,\n",
    "            \"random_state\": 42,\n",
    "            \"n_features\": X_train_df.shape[1],\n",
    "            \"n_samples_train\": X_train_df.shape[0],\n",
    "            \"n_samples_test\": X_test_df.shape[0]\n",
    "        })\n",
    "        mlflow.set_tag(\"team\", team_name)\n",
    "        mlflow.set_tag(\"pipeline\", \"solution\")\n",
    "        \n",
    "        # ëª¨ë¸ í•™ìŠµ\n",
    "        print(f\"  Training RandomForest...\")\n",
    "        model = RandomForestRegressor(\n",
    "            n_estimators=n_estimators,\n",
    "            max_depth=max_depth,\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        model.fit(X_train_df, y_train_df.values.ravel())\n",
    "        \n",
    "        # ì˜ˆì¸¡ ë° í‰ê°€\n",
    "        y_pred = model.predict(X_test_df)\n",
    "        \n",
    "        r2 = r2_score(y_test_df, y_pred)\n",
    "        rmse = np.sqrt(mean_squared_error(y_test_df, y_pred))\n",
    "        mae = mean_absolute_error(y_test_df, y_pred)\n",
    "        \n",
    "        # ë©”íŠ¸ë¦­ ë¡œê¹…\n",
    "        mlflow.log_metrics({\n",
    "            \"r2\": r2,\n",
    "            \"rmse\": rmse,\n",
    "            \"mae\": mae\n",
    "        })\n",
    "        \n",
    "        print(f\"  Performance:\")\n",
    "        print(f\"    - R2 Score: {r2:.4f}\")\n",
    "        print(f\"    - RMSE: {rmse:.4f}\")\n",
    "        print(f\"    - MAE: {mae:.4f}\")\n",
    "        \n",
    "        # í”¼ì²˜ ì¤‘ìš”ë„ (ìƒìœ„ 10ê°œ)\n",
    "        feature_importance = dict(zip(\n",
    "            X_train_df.columns,\n",
    "            model.feature_importances_\n",
    "        ))\n",
    "        sorted_importance = sorted(\n",
    "            feature_importance.items(),\n",
    "            key=lambda x: x[1],\n",
    "            reverse=True\n",
    "        )\n",
    "        \n",
    "        print(f\"  Feature Importance (Top 10):\")\n",
    "        for i, (feat, imp) in enumerate(sorted_importance[:10]):\n",
    "            safe_name = feat.replace(\" \", \"_\")[:20]\n",
    "            mlflow.log_metric(f\"fi_{safe_name}\", imp)\n",
    "            print(f\"    {i+1}. {feat}: {imp:.4f}\")\n",
    "        \n",
    "        # ëª¨ë¸ ì €ì¥\n",
    "        mlflow.sklearn.log_model(model, \"model\")\n",
    "        \n",
    "        print(f\"  âœ… Training completed\")\n",
    "    \n",
    "    return run_id\n",
    "\n",
    "print(\"âœ… train_model ì»´í¬ë„ŒíŠ¸ ì •ì˜ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Component 5: ëª¨ë¸ í‰ê°€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=\"python:3.9-slim\",\n",
    "    packages_to_install=[\"mlflow==2.9.2\"]\n",
    ")\n",
    "def evaluate_model(\n",
    "    run_id: str,\n",
    "    mlflow_tracking_uri: str,\n",
    "    r2_threshold: float = 0.75\n",
    ") -> str:\n",
    "    \"\"\"ëª¨ë¸ í‰ê°€ ë° ë°°í¬ ê²°ì •\"\"\"\n",
    "    import mlflow\n",
    "    import os\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"  Step 5: Evaluate Model\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    os.environ['MLFLOW_TRACKING_URI'] = mlflow_tracking_uri\n",
    "    mlflow.set_tracking_uri(mlflow_tracking_uri)\n",
    "    \n",
    "    client = mlflow.tracking.MlflowClient()\n",
    "    run = client.get_run(run_id)\n",
    "    \n",
    "    r2 = float(run.data.metrics.get(\"r2\", 0))\n",
    "    rmse = float(run.data.metrics.get(\"rmse\", 0))\n",
    "    mae = float(run.data.metrics.get(\"mae\", 0))\n",
    "    \n",
    "    print(f\"  Run ID: {run_id}\")\n",
    "    print(f\"  Metrics:\")\n",
    "    print(f\"    - R2 Score: {r2:.4f}\")\n",
    "    print(f\"    - RMSE: {rmse:.4f}\")\n",
    "    print(f\"    - MAE: {mae:.4f}\")\n",
    "    print(f\"  Threshold: R2 >= {r2_threshold}\")\n",
    "    \n",
    "    if r2 >= r2_threshold:\n",
    "        decision = \"deploy\"\n",
    "        print(f\"  âœ… Decision: DEPLOY\")\n",
    "    else:\n",
    "        decision = \"skip\"\n",
    "        print(f\"  âš ï¸ Decision: SKIP\")\n",
    "    \n",
    "    with mlflow.start_run(run_id=run_id):\n",
    "        mlflow.set_tag(\"deployment_decision\", decision)\n",
    "    \n",
    "    return decision\n",
    "\n",
    "print(\"âœ… evaluate_model ì»´í¬ë„ŒíŠ¸ ì •ì˜ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Component 6: ëª¨ë¸ ë°°í¬ (KServe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=\"python:3.9-slim\",\n",
    "    packages_to_install=[\"kubernetes==28.1.0\"]\n",
    ")\n",
    "def deploy_model(\n",
    "    run_id: str,\n",
    "    model_name: str,\n",
    "    namespace: str\n",
    "):\n",
    "    \"\"\"KServe InferenceServiceë¡œ ëª¨ë¸ ë°°í¬ (ì™„ì„± ë²„ì „)\"\"\"\n",
    "    from kubernetes import client, config\n",
    "    from kubernetes.client.rest import ApiException\n",
    "    import time\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"  Step 6: Deploy Model (KServe)\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    print(f\"  Model Name: {model_name}\")\n",
    "    print(f\"  Namespace: {namespace}\")\n",
    "    print(f\"  Run ID: {run_id}\")\n",
    "    \n",
    "    try:\n",
    "        config.load_incluster_config()\n",
    "    except:\n",
    "        config.load_kube_config()\n",
    "    \n",
    "    api = client.CustomObjectsApi()\n",
    "    \n",
    "    isvc = {\n",
    "        \"apiVersion\": \"serving.kserve.io/v1beta1\",\n",
    "        \"kind\": \"InferenceService\",\n",
    "        \"metadata\": {\n",
    "            \"name\": model_name,\n",
    "            \"namespace\": namespace,\n",
    "            \"annotations\": {\n",
    "                \"sidecar.istio.io/inject\": \"false\"\n",
    "            }\n",
    "        },\n",
    "        \"spec\": {\n",
    "            \"predictor\": {\n",
    "                \"sklearn\": {\n",
    "                    \"storageUri\": f\"mlflow-artifacts:/{run_id}/model\",\n",
    "                    \"resources\": {\n",
    "                        \"requests\": {\"cpu\": \"100m\", \"memory\": \"256Mi\"},\n",
    "                        \"limits\": {\"cpu\": \"500m\", \"memory\": \"512Mi\"}\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # ê¸°ì¡´ ì‚­ì œ\n",
    "    try:\n",
    "        api.delete_namespaced_custom_object(\n",
    "            \"serving.kserve.io\", \"v1beta1\", namespace, \"inferenceservices\", model_name\n",
    "        )\n",
    "        print(f\"  Deleted existing InferenceService\")\n",
    "        time.sleep(5)\n",
    "    except ApiException as e:\n",
    "        if e.status != 404:\n",
    "            raise\n",
    "    \n",
    "    # ìƒˆë¡œ ìƒì„±\n",
    "    api.create_namespaced_custom_object(\n",
    "        \"serving.kserve.io\", \"v1beta1\", namespace, \"inferenceservices\", isvc\n",
    "    )\n",
    "    print(f\"  âœ… InferenceService created\")\n",
    "    \n",
    "    # ìƒíƒœ í™•ì¸\n",
    "    print(f\"  Waiting for deployment...\")\n",
    "    for i in range(6):\n",
    "        time.sleep(10)\n",
    "        try:\n",
    "            status = api.get_namespaced_custom_object(\n",
    "                \"serving.kserve.io\", \"v1beta1\", namespace, \"inferenceservices\", model_name\n",
    "            )\n",
    "            conditions = status.get(\"status\", {}).get(\"conditions\", [])\n",
    "            ready = next((c for c in conditions if c.get(\"type\") == \"Ready\"), None)\n",
    "            if ready and ready.get(\"status\") == \"True\":\n",
    "                print(f\"  âœ… InferenceService READY!\")\n",
    "                break\n",
    "            print(f\"  â³ Status: {ready.get('status') if ready else 'Unknown'} ({(i+1)*10}s)\")\n",
    "        except Exception as e:\n",
    "            print(f\"  âš ï¸ Error: {e}\")\n",
    "    \n",
    "    print(f\"  Endpoint: http://{model_name}.{namespace}.svc.cluster.local/v1/models/{model_name}:predict\")\n",
    "    print(f\"  âœ… Deployment completed\")\n",
    "\n",
    "print(\"âœ… deploy_model ì»´í¬ë„ŒíŠ¸ ì •ì˜ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7 Component 7: ì•Œë¦¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(base_image=\"python:3.9-slim\")\n",
    "def send_alert(run_id: str, team_name: str):\n",
    "    \"\"\"ì„±ëŠ¥ ë¯¸ë‹¬ ì•Œë¦¼\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"  Alert - {team_name}\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"  âš ï¸ Model did not meet performance threshold\")\n",
    "    print(f\"  Run ID: {run_id}\")\n",
    "    print(f\"  Recommendations:\")\n",
    "    print(f\"    1. Add more features\")\n",
    "    print(f\"    2. Tune hyperparameters\")\n",
    "    print(f\"    3. Try different algorithms\")\n",
    "\n",
    "print(\"âœ… send_alert ì»´í¬ë„ŒíŠ¸ ì •ì˜ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. íŒŒì´í”„ë¼ì¸ ì •ì˜ (ì™„ì„± ë²„ì „)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def get_current_namespace():\n",
    "    \"\"\"í˜„ì¬ Podê°€ ì‹¤í–‰ ì¤‘ì¸ namespaceë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.\"\"\"\n",
    "    # ë°©ë²• 1: Kubernetes ServiceAccountì—ì„œ ì½ê¸°\n",
    "    namespace_path = \"/var/run/secrets/kubernetes.io/serviceaccount/namespace\"\n",
    "    if os.path.exists(namespace_path):\n",
    "        with open(namespace_path, \"r\") as f:\n",
    "            return f.read().strip()\n",
    "    \n",
    "    # ë°©ë²• 2: í™˜ê²½ë³€ìˆ˜ì—ì„œ ì½ê¸°\n",
    "    if os.environ.get(\"KF_PIPELINES_NAMESPACE\"):\n",
    "        return os.environ.get(\"KF_PIPELINES_NAMESPACE\")\n",
    "    \n",
    "    # ë°©ë²• 3: ê¸°ë³¸ê°’\n",
    "    return \"kubeflow-user\"\n",
    "\n",
    "# í˜„ì¬ namespace ê°€ì ¸ì˜¤ê¸°\n",
    "NAMESPACE = get_current_namespace()\n",
    "print(f\"í˜„ì¬ Namespace: {NAMESPACE}\")\n",
    "\n",
    "@dsl.pipeline(\n",
    "    name=\"Project Pipeline (Solution)\",\n",
    "    description=\"Solution: E2E ML Pipeline with 7 engineered features\"\n",
    ")\n",
    "def project_pipeline(\n",
    "    data_source: str = \"sklearn\",\n",
    "    team_name: str = \"solution-team\",\n",
    "    experiment_name: str = \"solution-experiment\",\n",
    "    model_name: str = \"solution-model\",\n",
    "    namespace: str = NAMESPACE,\n",
    "    mlflow_tracking_uri: str = \"http://mlflow-server-service.mlflow-system.svc.cluster.local:5000\",\n",
    "    n_estimators: int = 100,\n",
    "    max_depth: int = 10,\n",
    "    r2_threshold: float = 0.75\n",
    "):\n",
    "    \"\"\"í”„ë¡œì íŠ¸ ì†”ë£¨ì…˜ íŒŒì´í”„ë¼ì¸\"\"\"\n",
    "    \n",
    "    # Step 1: ë°ì´í„° ë¡œë“œ\n",
    "    load_task = load_data(data_source=data_source)\n",
    "    \n",
    "    # Step 2: ì „ì²˜ë¦¬\n",
    "    preprocess_task = preprocess(\n",
    "        input_data=load_task.outputs[\"output_data\"]\n",
    "    )\n",
    "    \n",
    "    # Step 3: í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ (7ê°œ í”¼ì²˜)\n",
    "    feature_task = feature_engineering(\n",
    "        X_train_in=preprocess_task.outputs[\"X_train_out\"],\n",
    "        X_test_in=preprocess_task.outputs[\"X_test_out\"]\n",
    "    )\n",
    "    \n",
    "    # Step 4: ëª¨ë¸ í•™ìŠµ\n",
    "    train_task = train_model(\n",
    "        X_train=feature_task.outputs[\"X_train_out\"],\n",
    "        X_test=feature_task.outputs[\"X_test_out\"],\n",
    "        y_train=preprocess_task.outputs[\"y_train_out\"],\n",
    "        y_test=preprocess_task.outputs[\"y_test_out\"],\n",
    "        mlflow_tracking_uri=mlflow_tracking_uri,\n",
    "        experiment_name=experiment_name,\n",
    "        team_name=team_name,\n",
    "        n_estimators=n_estimators,\n",
    "        max_depth=max_depth\n",
    "    )\n",
    "    \n",
    "    # Step 5: í‰ê°€\n",
    "    evaluate_task = evaluate_model(\n",
    "        run_id=train_task.output,\n",
    "        mlflow_tracking_uri=mlflow_tracking_uri,\n",
    "        r2_threshold=r2_threshold\n",
    "    )\n",
    "    \n",
    "    # Step 6: ì¡°ê±´ë¶€ ë°°í¬\n",
    "    with dsl.If(evaluate_task.output == \"deploy\"):\n",
    "        deploy_model(\n",
    "            run_id=train_task.output,\n",
    "            model_name=model_name,\n",
    "            namespace=namespace\n",
    "        )\n",
    "    \n",
    "    with dsl.If(evaluate_task.output == \"skip\"):\n",
    "        send_alert(\n",
    "            run_id=train_task.output,\n",
    "            team_name=team_name\n",
    "        )\n",
    "\n",
    "print(\"âœ… project_pipeline (Solution) ì •ì˜ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. íŒŒì´í”„ë¼ì¸ ì»´íŒŒì¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# íŒŒì´í”„ë¼ì¸ ì»´íŒŒì¼\n",
    "pipeline_file = \"project_solution_pipeline.yaml\"\n",
    "\n",
    "compiler.Compiler().compile(\n",
    "    pipeline_func=project_pipeline,\n",
    "    package_path=pipeline_file\n",
    ")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"  ì†”ë£¨ì…˜ íŒŒì´í”„ë¼ì¸ ì»´íŒŒì¼ ì™„ë£Œ!\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"  íŒŒì¼: {pipeline_file}\")\n",
    "print(\"\")\n",
    "print(\"  ì†”ë£¨ì…˜ í¬í•¨ ë‚´ìš©:\")\n",
    "print(\"    - 7ê°œ íŒŒìƒ í”¼ì²˜ (rooms_per_household, bedrooms_ratio, ...)\")\n",
    "print(\"    - ì™„ì „í•œ MLflow í†µí•©\")\n",
    "print(\"    - í”¼ì²˜ ì¤‘ìš”ë„ ë¡œê¹…\")\n",
    "print(\"    - KServe ë°°í¬ (ìƒíƒœ í™•ì¸ í¬í•¨)\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“Š ì†”ë£¨ì…˜ í”¼ì²˜ ìš”ì•½\n",
    "\n",
    "| # | í”¼ì²˜ëª… | ì„¤ëª… | ê³µì‹ |\n",
    "|---|--------|------|------|\n",
    "| 1 | rooms_per_household | ê°€êµ¬ë‹¹ ë°© ìˆ˜ | AveRooms / AveOccup |\n",
    "| 2 | bedrooms_ratio | ë°© ëŒ€ë¹„ ì¹¨ì‹¤ ë¹„ìœ¨ | AveBedrms / AveRooms |\n",
    "| 3 | population_per_household | ê°€êµ¬ë‹¹ ì¸êµ¬ | Population / AveOccup |\n",
    "| 4 | dist_to_bay | Bay Areaê¹Œì§€ ê±°ë¦¬ | âˆš(LatÂ² + LonÂ²) |\n",
    "| 5 | density | ë°€ì§‘ë„ ì§€í‘œ | Population Ã— AveOccup |\n",
    "| 6 | income_rooms | ì†Œë“Ã—ë°© ìƒí˜¸ì‘ìš© | MedInc Ã— AveRooms |\n",
    "| 7 | location_score | ìœ„ì¹˜ ì ìˆ˜ | LatÃ—0.5 + LonÃ—0.5 |\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ‰ ìˆ˜ê³ í•˜ì…¨ìŠµë‹ˆë‹¤!\n",
    "\n",
    "ì´ ì†”ë£¨ì…˜ì„ ì°¸ê³ í•˜ì—¬ ë³¸ì¸ì˜ íŒŒì´í”„ë¼ì¸ì„ ê°œì„ í•´ ë³´ì„¸ìš”.\n",
    "\n",
    "---\n",
    "\n",
    "**í˜„ëŒ€ì˜¤í† ì—ë²„ MLOps Training**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbformat_minor": 5,
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
