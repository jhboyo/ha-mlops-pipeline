{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéØ Day 3 Ï°∞Î≥Ñ ÌîÑÎ°úÏ†ùÌä∏: E2E MLOps Pipeline\n",
    "\n",
    "## üìã Í∞úÏöî\n",
    "\n",
    "3ÏùºÍ∞Ñ ÌïôÏäµÌïú MLOps Í∏∞Ïà†ÏùÑ Ï¢ÖÌï©ÌïòÏó¨ **ÏôÑÏ†ÑÌïú E2E ML ÌååÏù¥ÌîÑÎùºÏù∏**ÏùÑ Íµ¨Ï∂ïÌï©ÎãàÎã§.\n",
    "\n",
    "### ÌèâÍ∞Ä Í∏∞Ï§Ä\n",
    "\n",
    "| Ìï≠Î™© | Î∞∞Ï†ê | Í∏∞Ï§Ä |\n",
    "|------|------|------|\n",
    "| Kubeflow Pipeline | 40Ï†ê | ÏµúÏÜå 5Í∞ú Ïª¥Ìè¨ÎÑåÌä∏, Succeeded ÏÉÅÌÉú |\n",
    "| MLflow Tracking | 20Ï†ê | ÏµúÏÜå 2Ìöå Run, ÌååÎùºÎØ∏ÌÑ∞/Î©îÌä∏Î¶≠ Í∏∞Î°ù |\n",
    "| Feature Engineering | 10Ï†ê | 1Í∞ú Ïù¥ÏÉÅ ÌååÏÉù ÌîºÏ≤ò ÏÉùÏÑ± |\n",
    "| KServe Î∞∞Ìè¨ (ÏÑ†ÌÉù) | 25Ï†ê | InferenceService ÏÉùÏÑ± Î∞è API ÌÖåÏä§Ìä∏ |\n",
    "\n",
    "---\n",
    "\n",
    "### ÌååÏù¥ÌîÑÎùºÏù∏ Íµ¨Ï°∞\n",
    "\n",
    "```\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ  1. Load     ‚îÇ ‚îÄ‚îÄ‚ñ∂ ‚îÇ  2. Preprocess‚îÇ ‚îÄ‚îÄ‚ñ∂ ‚îÇ  3. Feature  ‚îÇ\n",
    "‚îÇ     Data     ‚îÇ     ‚îÇ              ‚îÇ     ‚îÇ   Engineering‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                                                  ‚îÇ\n",
    "                                                  ‚ñº\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ  6. Deploy   ‚îÇ ‚óÄ‚îÄ‚îÄ ‚îÇ  5. Evaluate ‚îÇ ‚óÄ‚îÄ‚îÄ ‚îÇ  4. Train    ‚îÇ\n",
    "‚îÇ   (KServe)   ‚îÇ     ‚îÇ              ‚îÇ     ‚îÇ     Model    ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. ÌôòÍ≤Ω ÏÑ§Ï†ï"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ìå®ÌÇ§ÏßÄ ÏÑ§Ïπò\n",
    "!pip install kfp==2.7.0 mlflow>=2.9.0 scikit-learn pandas numpy kubernetes -q\n",
    "print(\"‚úÖ Ìå®ÌÇ§ÏßÄ ÏÑ§Ïπò ÏôÑÎ£å!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import\n",
    "import os\n",
    "from kfp import dsl\n",
    "from kfp.dsl import component, Input, Output, Dataset, Model, Metrics\n",
    "from kfp import compiler\n",
    "import kfp\n",
    "\n",
    "print(f\"KFP Version: {kfp.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ‚ö†Ô∏è TODO: ÌåÄ ÏÑ§Ï†ïÏúºÎ°ú Î≥ÄÍ≤ΩÌïòÏÑ∏Ïöî!\n",
    "# ============================================================\n",
    "TEAM_NAME = \"team-XX\"                        # Ïòà: team-01, team-02, ..., team-06\n",
    "\n",
    "# ÎÑ§ÏûÑÏä§ÌéòÏù¥Ïä§ ÏûêÎèô Í∞êÏßÄ ÎòêÎäî ÌôòÍ≤Ω Î≥ÄÏàò Í∏∞Î∞ò ÏÑ§Ï†ï\n",
    "def get_namespace():\n",
    "    try:\n",
    "        with open('/var/run/secrets/kubernetes.io/serviceaccount/namespace', 'r') as f:\n",
    "            return f.read().strip()\n",
    "    except FileNotFoundError:\n",
    "        user_num = os.getenv('USER_NUM', '01')\n",
    "        return f\"kubeflow-user{user_num}\"\n",
    "\n",
    "USER_NAMESPACE = get_namespace()\n",
    "\n",
    "# MLflow ÏÑ§Ï†ï (Î≥ÄÍ≤Ω Î∂àÌïÑÏöî)\n",
    "MLFLOW_TRACKING_URI = \"http://mlflow-server-service.mlflow-system.svc.cluster.local:5000\"\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"  ÌåÄ ÏÑ§Ï†ï\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"  Team: {TEAM_NAME}\")\n",
    "print(f\"  Namespace: {USER_NAMESPACE}\")\n",
    "print(f\"  MLflow URI: {MLFLOW_TRACKING_URI}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Ïª¥Ìè¨ÎÑåÌä∏ Ï†ïÏùò\n",
    "\n",
    "### 2.1 Component 1: Îç∞Ïù¥ÌÑ∞ Î°úÎìú"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=\"python:3.9-slim\",\n",
    "    packages_to_install=[\"pandas==2.0.3\", \"scikit-learn==1.3.2\"]\n",
    ")\n",
    "def load_data(\n",
    "    data_source: str,\n",
    "    output_data: Output[Dataset]\n",
    "):\n",
    "    \"\"\"\n",
    "    California Housing Îç∞Ïù¥ÌÑ∞ÏÖã Î°úÎìú\n",
    "    \n",
    "    Args:\n",
    "        data_source: Îç∞Ïù¥ÌÑ∞ ÏÜåÏä§ (sklearn)\n",
    "        output_data: Ï∂úÎ†• Îç∞Ïù¥ÌÑ∞ÏÖã Í≤ΩÎ°ú\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    from sklearn.datasets import fetch_california_housing\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"  Step 1: Load Data\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # California Housing Îç∞Ïù¥ÌÑ∞ÏÖã Î°úÎìú\n",
    "    housing = fetch_california_housing(as_frame=True)\n",
    "    df = housing.frame\n",
    "    \n",
    "    print(f\"  Source: {data_source}\")\n",
    "    print(f\"  Shape: {df.shape}\")\n",
    "    print(f\"  Columns: {list(df.columns)}\")\n",
    "    print(f\"  Target: MedHouseVal (Median House Value in $100k)\")\n",
    "    \n",
    "    # CSVÎ°ú Ï†ÄÏû•\n",
    "    df.to_csv(output_data.path, index=False)\n",
    "    print(f\"  ‚úÖ Data saved: {output_data.path}\")\n",
    "\n",
    "print(\"‚úÖ load_data Ïª¥Ìè¨ÎÑåÌä∏ Ï†ïÏùò ÏôÑÎ£å!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Component 2: Ï†ÑÏ≤òÎ¶¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=\"python:3.9-slim\",\n",
    "    packages_to_install=[\"pandas==2.0.3\", \"scikit-learn==1.3.2\", \"numpy==1.24.3\"]\n",
    ")\n",
    "def preprocess(\n",
    "    input_data: Input[Dataset],\n",
    "    X_train_out: Output[Dataset],\n",
    "    X_test_out: Output[Dataset],\n",
    "    y_train_out: Output[Dataset],\n",
    "    y_test_out: Output[Dataset],\n",
    "    test_size: float = 0.2\n",
    "):\n",
    "    \"\"\"\n",
    "    Îç∞Ïù¥ÌÑ∞ Ï†ÑÏ≤òÎ¶¨: Train/Test Î∂ÑÌï† Î∞è Ï†ïÍ∑úÌôî\n",
    "    \n",
    "    Args:\n",
    "        input_data: ÏûÖÎ†• Îç∞Ïù¥ÌÑ∞ÏÖã\n",
    "        X_train_out, X_test_out: ÌîºÏ≤ò Ï∂úÎ†•\n",
    "        y_train_out, y_test_out: ÌÉÄÍ≤ü Ï∂úÎ†•\n",
    "        test_size: ÌÖåÏä§Ìä∏ Îç∞Ïù¥ÌÑ∞ ÎπÑÏú®\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"  Step 2: Preprocess\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Îç∞Ïù¥ÌÑ∞ Î°úÎìú\n",
    "    df = pd.read_csv(input_data.path)\n",
    "    print(f\"  Loaded {len(df)} rows\")\n",
    "    \n",
    "    # ÌîºÏ≤òÏôÄ ÌÉÄÍ≤ü Î∂ÑÎ¶¨\n",
    "    X = df.drop(columns=['MedHouseVal'])\n",
    "    y = df['MedHouseVal']\n",
    "    \n",
    "    # Train/Test Î∂ÑÌï†\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=42\n",
    "    )\n",
    "    \n",
    "    # StandardScalerÎ°ú Ï†ïÍ∑úÌôî\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = pd.DataFrame(\n",
    "        scaler.fit_transform(X_train),\n",
    "        columns=X_train.columns\n",
    "    )\n",
    "    X_test_scaled = pd.DataFrame(\n",
    "        scaler.transform(X_test),\n",
    "        columns=X_test.columns\n",
    "    )\n",
    "    \n",
    "    # Ï†ÄÏû•\n",
    "    X_train_scaled.to_csv(X_train_out.path, index=False)\n",
    "    X_test_scaled.to_csv(X_test_out.path, index=False)\n",
    "    y_train.to_csv(y_train_out.path, index=False)\n",
    "    y_test.to_csv(y_test_out.path, index=False)\n",
    "    \n",
    "    print(f\"  Train: {len(X_train)} samples\")\n",
    "    print(f\"  Test: {len(X_test)} samples\")\n",
    "    print(f\"  ‚úÖ Preprocessing completed\")\n",
    "\n",
    "print(\"‚úÖ preprocess Ïª¥Ìè¨ÎÑåÌä∏ Ï†ïÏùò ÏôÑÎ£å!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Component 3: ÌîºÏ≤ò ÏóîÏßÄÎãàÏñ¥ÎßÅ\n",
    "\n",
    "‚ö†Ô∏è **TODO: ÏµúÏÜå 1Í∞ú Ïù¥ÏÉÅÏùò ÌååÏÉù ÌîºÏ≤òÎ•º Ï∂îÍ∞ÄÌïòÏÑ∏Ïöî!**\n",
    "\n",
    "#### ÌîºÏ≤ò ÏïÑÏù¥ÎîîÏñ¥:\n",
    "```python\n",
    "# 1. Ïπ®Ïã§ ÎπÑÏú®\n",
    "df['bedroom_ratio'] = df['AveBedrms'] / (df['AveRooms'] + 1e-6)\n",
    "\n",
    "# 2. Ïù∏Îãπ Î∞© Ïàò\n",
    "df['rooms_per_person'] = df['AveRooms'] / (df['AveOccup'] + 1e-6)\n",
    "\n",
    "# 3. Î∞ÄÏßëÎèÑ\n",
    "df['density'] = df['Population'] * df['AveOccup']\n",
    "\n",
    "# 4. ÏÜåÎìù √ó Î∞© Ïàò ÏÉÅÌò∏ÏûëÏö©\n",
    "df['income_rooms'] = df['MedInc'] * df['AveRooms']\n",
    "\n",
    "# 5. Bay AreaÍπåÏßÄ Í±∞Î¶¨\n",
    "df['dist_to_bay'] = np.sqrt(df['Latitude']**2 + df['Longitude']**2)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=\"python:3.9-slim\",\n",
    "    packages_to_install=[\"pandas==2.0.3\", \"numpy==1.24.3\"]\n",
    ")\n",
    "def feature_engineering(\n",
    "    X_train_in: Input[Dataset],\n",
    "    X_test_in: Input[Dataset],\n",
    "    X_train_out: Output[Dataset],\n",
    "    X_test_out: Output[Dataset]\n",
    ") -> int:\n",
    "    \"\"\"\n",
    "    ÌîºÏ≤ò ÏóîÏßÄÎãàÏñ¥ÎßÅ: ÌååÏÉù Î≥ÄÏàò ÏÉùÏÑ±\n",
    "    \n",
    "    Returns:\n",
    "        int: ÏÉùÏÑ±Îêú ÌîºÏ≤ò Ïàò\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"  Step 3: Feature Engineering\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Îç∞Ïù¥ÌÑ∞ Î°úÎìú\n",
    "    X_train = pd.read_csv(X_train_in.path)\n",
    "    X_test = pd.read_csv(X_test_in.path)\n",
    "    \n",
    "    original_cols = list(X_train.columns)\n",
    "    print(f\"  Original features: {len(original_cols)}\")\n",
    "    \n",
    "    def add_features(df):\n",
    "        \"\"\"ÌååÏÉù Î≥ÄÏàò Ï∂îÍ∞Ä\"\"\"\n",
    "        df = df.copy()\n",
    "        \n",
    "        # ============================================================\n",
    "        # ‚ö†Ô∏è TODO: Ïó¨Í∏∞Ïóê ÌååÏÉù Î≥ÄÏàòÎ•º Ï∂îÍ∞ÄÌïòÏÑ∏Ïöî! (ÏµúÏÜå 1Í∞ú Ïù¥ÏÉÅ)\n",
    "        # ============================================================\n",
    "        \n",
    "        # ÏòàÏãú 1: Ïπ®Ïã§ ÎπÑÏú®\n",
    "        df['bedroom_ratio'] = df['AveBedrms'] / (df['AveRooms'] + 1e-6)\n",
    "        \n",
    "        # ÏòàÏãú 2: TODO - Ï∂îÍ∞Ä ÌîºÏ≤òÎ•º Íµ¨ÌòÑÌïòÏÑ∏Ïöî!\n",
    "        # df['rooms_per_person'] = ...\n",
    "        \n",
    "        # ÏòàÏãú 3: TODO - Ï∂îÍ∞Ä ÌîºÏ≤òÎ•º Íµ¨ÌòÑÌïòÏÑ∏Ïöî!\n",
    "        # df['density'] = ...\n",
    "        \n",
    "        # ============================================================\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    # ÌîºÏ≤ò Ï∂îÍ∞Ä\n",
    "    X_train_fe = add_features(X_train)\n",
    "    X_test_fe = add_features(X_test)\n",
    "    \n",
    "    # ÏÉàÎ°ú ÏÉùÏÑ±Îêú ÌîºÏ≤ò ÌôïÏù∏\n",
    "    new_cols = [c for c in X_train_fe.columns if c not in original_cols]\n",
    "    print(f\"  New features ({len(new_cols)}):\")\n",
    "    for feat in new_cols:\n",
    "        stats = X_train_fe[feat].describe()\n",
    "        print(f\"    - {feat}: mean={stats['mean']:.4f}, std={stats['std']:.4f}\")\n",
    "    \n",
    "    print(f\"  Total features: {len(X_train_fe.columns)}\")\n",
    "    \n",
    "    # Ï†ÄÏû•\n",
    "    X_train_fe.to_csv(X_train_out.path, index=False)\n",
    "    X_test_fe.to_csv(X_test_out.path, index=False)\n",
    "    \n",
    "    print(f\"  ‚úÖ Feature engineering completed\")\n",
    "    \n",
    "    return len(new_cols)\n",
    "\n",
    "print(\"‚úÖ feature_engineering Ïª¥Ìè¨ÎÑåÌä∏ Ï†ïÏùò ÏôÑÎ£å!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Component 4: Î™®Îç∏ ÌïôÏäµ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=\"python:3.9-slim\",\n",
    "    packages_to_install=[\n",
    "        \"pandas==2.0.3\",\n",
    "        \"scikit-learn==1.3.2\",\n",
    "        \"mlflow==2.9.2\",\n",
    "        \"numpy==1.24.3\",\n",
    "        \"boto3\"\n",
    "    ]\n",
    ")\n",
    "def train_model(\n",
    "    X_train: Input[Dataset],\n",
    "    X_test: Input[Dataset],\n",
    "    y_train: Input[Dataset],\n",
    "    y_test: Input[Dataset],\n",
    "    mlflow_tracking_uri: str,\n",
    "    experiment_name: str,\n",
    "    team_name: str,\n",
    "    n_estimators: int = 100,\n",
    "    max_depth: int = 10\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Î™®Îç∏ ÌïôÏäµ Î∞è MLflow Í∏∞Î°ù\n",
    "    \n",
    "    Returns:\n",
    "        str: MLflow Run ID\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import mlflow\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "    import os\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(f\"  Step 4: Train Model - {team_name}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Îç∞Ïù¥ÌÑ∞ Î°úÎìú\n",
    "    X_train_df = pd.read_csv(X_train.path)\n",
    "    X_test_df = pd.read_csv(X_test.path)\n",
    "    y_train_df = pd.read_csv(y_train.path)\n",
    "    y_test_df = pd.read_csv(y_test.path)\n",
    "    \n",
    "    print(f\"  Training data: {X_train_df.shape}\")\n",
    "    print(f\"  Test data: {X_test_df.shape}\")\n",
    "    \n",
    "    # MLflow ÏÑ§Ï†ï\n",
    "    os.environ['MLFLOW_TRACKING_URI'] = mlflow_tracking_uri\n",
    "    mlflow.set_tracking_uri(mlflow_tracking_uri)\n",
    "    mlflow.set_experiment(experiment_name)\n",
    "    \n",
    "    with mlflow.start_run(run_name=f\"{team_name}-run\") as run:\n",
    "        run_id = run.info.run_id\n",
    "        print(f\"  MLflow Run ID: {run_id}\")\n",
    "        \n",
    "        # ÌååÎùºÎØ∏ÌÑ∞ Î°úÍπÖ\n",
    "        mlflow.log_params({\n",
    "            \"n_estimators\": n_estimators,\n",
    "            \"max_depth\": max_depth,\n",
    "            \"random_state\": 42,\n",
    "            \"n_features\": X_train_df.shape[1]\n",
    "        })\n",
    "        mlflow.set_tag(\"team\", team_name)\n",
    "        \n",
    "        # Î™®Îç∏ ÌïôÏäµ\n",
    "        print(f\"  Training RandomForest (n_estimators={n_estimators}, max_depth={max_depth})...\")\n",
    "        model = RandomForestRegressor(\n",
    "            n_estimators=n_estimators,\n",
    "            max_depth=max_depth,\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        model.fit(X_train_df, y_train_df.values.ravel())\n",
    "        \n",
    "        # ÏòàÏ∏° Î∞è ÌèâÍ∞Ä\n",
    "        y_pred = model.predict(X_test_df)\n",
    "        \n",
    "        r2 = r2_score(y_test_df, y_pred)\n",
    "        rmse = np.sqrt(mean_squared_error(y_test_df, y_pred))\n",
    "        mae = mean_absolute_error(y_test_df, y_pred)\n",
    "        \n",
    "        # Î©îÌä∏Î¶≠ Î°úÍπÖ\n",
    "        mlflow.log_metrics({\n",
    "            \"r2\": r2,\n",
    "            \"rmse\": rmse,\n",
    "            \"mae\": mae\n",
    "        })\n",
    "        \n",
    "        print(f\"  Performance:\")\n",
    "        print(f\"    - R2 Score: {r2:.4f}\")\n",
    "        print(f\"    - RMSE: {rmse:.4f}\")\n",
    "        print(f\"    - MAE: {mae:.4f}\")\n",
    "        \n",
    "        # ÌîºÏ≤ò Ï§ëÏöîÎèÑ ÏÉÅÏúÑ 5Í∞ú\n",
    "        feature_importance = dict(zip(\n",
    "            X_train_df.columns,\n",
    "            model.feature_importances_\n",
    "        ))\n",
    "        sorted_importance = sorted(\n",
    "            feature_importance.items(),\n",
    "            key=lambda x: x[1],\n",
    "            reverse=True\n",
    "        )[:5]\n",
    "        \n",
    "        print(f\"  Top 5 Feature Importance:\")\n",
    "        for feat, imp in sorted_importance:\n",
    "            print(f\"    - {feat}: {imp:.4f}\")\n",
    "        \n",
    "        # Î™®Îç∏ Ï†ÄÏû•\n",
    "        mlflow.sklearn.log_model(model, \"model\")\n",
    "        \n",
    "        print(f\"  ‚úÖ Training completed\")\n",
    "    \n",
    "    return run_id\n",
    "\n",
    "print(\"‚úÖ train_model Ïª¥Ìè¨ÎÑåÌä∏ Ï†ïÏùò ÏôÑÎ£å!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Component 5: Î™®Îç∏ ÌèâÍ∞Ä"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=\"python:3.9-slim\",\n",
    "    packages_to_install=[\"mlflow==2.9.2\"]\n",
    ")\n",
    "def evaluate_model(\n",
    "    run_id: str,\n",
    "    mlflow_tracking_uri: str,\n",
    "    r2_threshold: float = 0.75\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Î™®Îç∏ ÌèâÍ∞Ä Î∞è Î∞∞Ìè¨ Í≤∞Ï†ï\n",
    "    \n",
    "    Returns:\n",
    "        str: \"deploy\" ÎòêÎäî \"skip\"\n",
    "    \"\"\"\n",
    "    import mlflow\n",
    "    import os\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"  Step 5: Evaluate Model\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # MLflow ÏÑ§Ï†ï\n",
    "    os.environ['MLFLOW_TRACKING_URI'] = mlflow_tracking_uri\n",
    "    mlflow.set_tracking_uri(mlflow_tracking_uri)\n",
    "    \n",
    "    # Î©îÌä∏Î¶≠ Ï°∞Ìöå\n",
    "    client = mlflow.tracking.MlflowClient()\n",
    "    run = client.get_run(run_id)\n",
    "    \n",
    "    r2 = float(run.data.metrics.get(\"r2\", 0))\n",
    "    rmse = float(run.data.metrics.get(\"rmse\", 0))\n",
    "    mae = float(run.data.metrics.get(\"mae\", 0))\n",
    "    \n",
    "    print(f\"  Run ID: {run_id}\")\n",
    "    print(f\"  Metrics:\")\n",
    "    print(f\"    - R2 Score: {r2:.4f}\")\n",
    "    print(f\"    - RMSE: {rmse:.4f}\")\n",
    "    print(f\"    - MAE: {mae:.4f}\")\n",
    "    print(f\"  Threshold: R2 >= {r2_threshold}\")\n",
    "    \n",
    "    # Î∞∞Ìè¨ Í≤∞Ï†ï\n",
    "    if r2 >= r2_threshold:\n",
    "        decision = \"deploy\"\n",
    "        print(f\"  ‚úÖ Decision: DEPLOY (R2 >= threshold)\")\n",
    "    else:\n",
    "        decision = \"skip\"\n",
    "        print(f\"  ‚ö†Ô∏è Decision: SKIP (R2 < threshold)\")\n",
    "    \n",
    "    # Í≤∞Ï†ï ÌÉúÍ∑∏ Ï†ÄÏû•\n",
    "    with mlflow.start_run(run_id=run_id):\n",
    "        mlflow.set_tag(\"deployment_decision\", decision)\n",
    "    \n",
    "    return decision\n",
    "\n",
    "print(\"‚úÖ evaluate_model Ïª¥Ìè¨ÎÑåÌä∏ Ï†ïÏùò ÏôÑÎ£å!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Component 6: Î™®Îç∏ Î∞∞Ìè¨ (KServe) - ÏÑ†ÌÉùÏÇ¨Ìï≠"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=\"python:3.9-slim\",\n",
    "    packages_to_install=[\"kubernetes==28.1.0\"]\n",
    ")\n",
    "def deploy_model(\n",
    "    run_id: str,\n",
    "    model_name: str,\n",
    "    namespace: str\n",
    "):\n",
    "    \"\"\"\n",
    "    KServe InferenceServiceÎ°ú Î™®Îç∏ Î∞∞Ìè¨\n",
    "    \n",
    "    Note: Ïù¥ Ïª¥Ìè¨ÎÑåÌä∏Îäî ÏÑ†ÌÉùÏÇ¨Ìï≠ÏûÖÎãàÎã§ (25Ï†ê)\n",
    "    \"\"\"\n",
    "    from kubernetes import client, config\n",
    "    from kubernetes.client.rest import ApiException\n",
    "    import time\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"  Step 6: Deploy Model (KServe)\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    print(f\"  Model Name: {model_name}\")\n",
    "    print(f\"  Namespace: {namespace}\")\n",
    "    print(f\"  Run ID: {run_id}\")\n",
    "    \n",
    "    # Kubernetes ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏ ÏÑ§Ï†ï\n",
    "    try:\n",
    "        config.load_incluster_config()\n",
    "    except:\n",
    "        config.load_kube_config()\n",
    "    \n",
    "    api = client.CustomObjectsApi()\n",
    "    \n",
    "    # InferenceService Ï†ïÏùò\n",
    "    isvc = {\n",
    "        \"apiVersion\": \"serving.kserve.io/v1beta1\",\n",
    "        \"kind\": \"InferenceService\",\n",
    "        \"metadata\": {\n",
    "            \"name\": model_name,\n",
    "            \"namespace\": namespace,\n",
    "            \"annotations\": {\n",
    "                \"sidecar.istio.io/inject\": \"false\"\n",
    "            }\n",
    "        },\n",
    "        \"spec\": {\n",
    "            \"predictor\": {\n",
    "                \"sklearn\": {\n",
    "                    \"storageUri\": f\"mlflow-artifacts:/{run_id}/model\",\n",
    "                    \"resources\": {\n",
    "                        \"requests\": {\"cpu\": \"100m\", \"memory\": \"256Mi\"},\n",
    "                        \"limits\": {\"cpu\": \"500m\", \"memory\": \"512Mi\"}\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Í∏∞Ï°¥ InferenceService ÏÇ≠Ï†ú (ÏûàÎäî Í≤ΩÏö∞)\n",
    "    try:\n",
    "        api.delete_namespaced_custom_object(\n",
    "            group=\"serving.kserve.io\",\n",
    "            version=\"v1beta1\",\n",
    "            namespace=namespace,\n",
    "            plural=\"inferenceservices\",\n",
    "            name=model_name\n",
    "        )\n",
    "        print(f\"  Deleted existing InferenceService\")\n",
    "        time.sleep(5)\n",
    "    except ApiException as e:\n",
    "        if e.status != 404:\n",
    "            raise\n",
    "    \n",
    "    # ÏÉà InferenceService ÏÉùÏÑ±\n",
    "    api.create_namespaced_custom_object(\n",
    "        group=\"serving.kserve.io\",\n",
    "        version=\"v1beta1\",\n",
    "        namespace=namespace,\n",
    "        plural=\"inferenceservices\",\n",
    "        body=isvc\n",
    "    )\n",
    "    \n",
    "    print(f\"  ‚úÖ InferenceService created\")\n",
    "    \n",
    "    # Î∞∞Ìè¨ ÏÉÅÌÉú ÌôïÏù∏ (ÏµúÎåÄ 60Ï¥à)\n",
    "    print(f\"  Waiting for deployment...\")\n",
    "    for i in range(6):\n",
    "        time.sleep(10)\n",
    "        try:\n",
    "            status = api.get_namespaced_custom_object(\n",
    "                group=\"serving.kserve.io\",\n",
    "                version=\"v1beta1\",\n",
    "                namespace=namespace,\n",
    "                plural=\"inferenceservices\",\n",
    "                name=model_name\n",
    "            )\n",
    "            conditions = status.get(\"status\", {}).get(\"conditions\", [])\n",
    "            ready = next((c for c in conditions if c.get(\"type\") == \"Ready\"), None)\n",
    "            if ready and ready.get(\"status\") == \"True\":\n",
    "                print(f\"  ‚úÖ InferenceService READY!\")\n",
    "                break\n",
    "            print(f\"  ‚è≥ Status: {ready.get('status') if ready else 'Unknown'} ({(i+1)*10}s)\")\n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ö†Ô∏è Error checking status: {e}\")\n",
    "    \n",
    "    print(f\"  Endpoint: http://{model_name}.{namespace}.svc.cluster.local/v1/models/{model_name}:predict\")\n",
    "    print(f\"  ‚úÖ Deployment completed\")\n",
    "\n",
    "print(\"‚úÖ deploy_model Ïª¥Ìè¨ÎÑåÌä∏ Ï†ïÏùò ÏôÑÎ£å!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7 Component 7: ÏïåÎ¶º (ÏÑ±Îä• ÎØ∏Îã¨ Ïãú)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(base_image=\"python:3.9-slim\")\n",
    "def send_alert(\n",
    "    run_id: str,\n",
    "    team_name: str\n",
    "):\n",
    "    \"\"\"\n",
    "    ÏÑ±Îä• ÎØ∏Îã¨ ÏïåÎ¶º\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"  Alert - {team_name}\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"  ‚ö†Ô∏è Model did not meet performance threshold\")\n",
    "    print(f\"  Run ID: {run_id}\")\n",
    "    print(f\"  Recommendations:\")\n",
    "    print(f\"    1. Add more features\")\n",
    "    print(f\"    2. Tune hyperparameters\")\n",
    "    print(f\"    3. Try different algorithms\")\n",
    "\n",
    "print(\"‚úÖ send_alert Ïª¥Ìè¨ÎÑåÌä∏ Ï†ïÏùò ÏôÑÎ£å!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. ÌååÏù¥ÌîÑÎùºÏù∏ Ï†ïÏùò"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "    name=f\"{TEAM_NAME}-pipeline\",\n",
    "    description=f\"E2E ML Pipeline by {TEAM_NAME}\"\n",
    ")\n",
    "def project_pipeline(\n",
    "    data_source: str = \"sklearn\",\n",
    "    team_name: str = TEAM_NAME,\n",
    "    experiment_name: str = f\"{TEAM_NAME}-experiment\",\n",
    "    model_name: str = f\"{TEAM_NAME}-model\",\n",
    "    namespace: str = USER_NAMESPACE,\n",
    "    mlflow_tracking_uri: str = MLFLOW_TRACKING_URI,\n",
    "    n_estimators: int = 100,\n",
    "    max_depth: int = 10,\n",
    "    r2_threshold: float = 0.75\n",
    "):\n",
    "    \"\"\"\n",
    "    E2E ML Pipeline\n",
    "    \n",
    "    Flow:\n",
    "    1. Load Data\n",
    "    2. Preprocess\n",
    "    3. Feature Engineering\n",
    "    4. Train Model (with MLflow)\n",
    "    5. Evaluate Model\n",
    "    6. Deploy (if R2 >= threshold) or Alert (if R2 < threshold)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Step 1: Îç∞Ïù¥ÌÑ∞ Î°úÎìú\n",
    "    load_task = load_data(data_source=data_source)\n",
    "    \n",
    "    # Step 2: Ï†ÑÏ≤òÎ¶¨\n",
    "    preprocess_task = preprocess(\n",
    "        input_data=load_task.outputs[\"output_data\"]\n",
    "    )\n",
    "    \n",
    "    # Step 3: ÌîºÏ≤ò ÏóîÏßÄÎãàÏñ¥ÎßÅ\n",
    "    feature_task = feature_engineering(\n",
    "        X_train_in=preprocess_task.outputs[\"X_train_out\"],\n",
    "        X_test_in=preprocess_task.outputs[\"X_test_out\"]\n",
    "    )\n",
    "    \n",
    "    # Step 4: Î™®Îç∏ ÌïôÏäµ\n",
    "    train_task = train_model(\n",
    "        X_train=feature_task.outputs[\"X_train_out\"],\n",
    "        X_test=feature_task.outputs[\"X_test_out\"],\n",
    "        y_train=preprocess_task.outputs[\"y_train_out\"],\n",
    "        y_test=preprocess_task.outputs[\"y_test_out\"],\n",
    "        mlflow_tracking_uri=mlflow_tracking_uri,\n",
    "        experiment_name=experiment_name,\n",
    "        team_name=team_name,\n",
    "        n_estimators=n_estimators,\n",
    "        max_depth=max_depth\n",
    "    )\n",
    "    \n",
    "    # Step 5: ÌèâÍ∞Ä\n",
    "    evaluate_task = evaluate_model(\n",
    "        run_id=train_task.output,\n",
    "        mlflow_tracking_uri=mlflow_tracking_uri,\n",
    "        r2_threshold=r2_threshold\n",
    "    )\n",
    "    \n",
    "    # Step 6: Ï°∞Í±¥Î∂Ä Î∞∞Ìè¨ ÎòêÎäî ÏïåÎ¶º\n",
    "    with dsl.If(evaluate_task.output == \"deploy\"):\n",
    "        deploy_model(\n",
    "            run_id=train_task.output,\n",
    "            model_name=model_name,\n",
    "            namespace=namespace\n",
    "        )\n",
    "    \n",
    "    with dsl.If(evaluate_task.output == \"skip\"):\n",
    "        send_alert(\n",
    "            run_id=train_task.output,\n",
    "            team_name=team_name\n",
    "        )\n",
    "\n",
    "print(\"‚úÖ project_pipeline Ï†ïÏùò ÏôÑÎ£å!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. ÌååÏù¥ÌîÑÎùºÏù∏ Ïª¥ÌååÏùº"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ÌååÏù¥ÌîÑÎùºÏù∏ Ïª¥ÌååÏùº\n",
    "pipeline_file = f\"{TEAM_NAME}_pipeline.yaml\"\n",
    "\n",
    "compiler.Compiler().compile(\n",
    "    pipeline_func=project_pipeline,\n",
    "    package_path=pipeline_file\n",
    ")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"  ÌååÏù¥ÌîÑÎùºÏù∏ Ïª¥ÌååÏùº ÏôÑÎ£å!\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"  ÌååÏùº: {pipeline_file}\")\n",
    "print(f\"  ÌåÄ: {TEAM_NAME}\")\n",
    "print(\"\")\n",
    "print(\"  Îã§Ïùå Îã®Í≥Ñ:\")\n",
    "print(\"  1. Kubeflow Dashboard ‚Üí Pipelines ‚Üí Upload pipeline\")\n",
    "print(f\"  2. {pipeline_file} ÏóÖÎ°úÎìú\")\n",
    "print(\"  3. Create run ‚Üí Start\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. ÌååÏù¥ÌîÑÎùºÏù∏ Ïã§Ìñâ (SDK Î∞©Ïãù - ÏÑ†ÌÉùÏÇ¨Ìï≠)\n",
    "\n",
    "UI ÎåÄÏã† SDKÎ°ú ÏßÅÏ†ë Ïã§ÌñâÌï† Ïàò ÏûàÏäµÎãàÎã§."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SDKÎ°ú ÌååÏù¥ÌîÑÎùºÏù∏ Ïã§Ìñâ (ÏÑ†ÌÉùÏÇ¨Ìï≠)\n",
    "# UIÏóêÏÑú Ïã§ÌñâÌïòÎäî Í≤ÉÏùÑ Í∂åÏû•Ìï©ÎãàÎã§.\n",
    "\n",
    "# from kfp.client import Client\n",
    "# \n",
    "# client = Client()\n",
    "# \n",
    "# run = client.create_run_from_pipeline_func(\n",
    "#     pipeline_func=project_pipeline,\n",
    "#     arguments={\n",
    "#         \"team_name\": TEAM_NAME,\n",
    "#         \"experiment_name\": f\"{TEAM_NAME}-experiment\",\n",
    "#         \"model_name\": f\"{TEAM_NAME}-model\",\n",
    "#         \"namespace\": USER_NAMESPACE,\n",
    "#         \"n_estimators\": 100,\n",
    "#         \"max_depth\": 10,\n",
    "#         \"r2_threshold\": 0.75\n",
    "#     },\n",
    "#     experiment_name=f\"{TEAM_NAME}-experiment\"\n",
    "# )\n",
    "# \n",
    "# print(f\"Run ID: {run.run_id}\")\n",
    "\n",
    "print(\"üí° SDK Ïã§Ìñâ ÏΩîÎìúÎäî Ï£ºÏÑù Ï≤òÎ¶¨ÎêòÏñ¥ ÏûàÏäµÎãàÎã§.\")\n",
    "print(\"   UIÏóêÏÑú ÌååÏù¥ÌîÑÎùºÏù∏ÏùÑ ÏóÖÎ°úÎìúÌïòÍ≥† Ïã§ÌñâÌïòÏÑ∏Ïöî.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Í≤∞Í≥º ÌôïÏù∏\n",
    "\n",
    "### 6.1 MLflow UIÏóêÏÑú ÌôïÏù∏\n",
    "\n",
    "```\n",
    "http://mlflow-server-service.mlflow-system.svc.cluster.local:5000\n",
    "```\n",
    "\n",
    "ÌôïÏù∏ Ìï≠Î™©:\n",
    "- Experiment: `{TEAM_NAME}-experiment`\n",
    "- Run: `{TEAM_NAME}-run`\n",
    "- Parameters: n_estimators, max_depth, n_features\n",
    "- Metrics: r2, rmse, mae\n",
    "- Artifacts: model\n",
    "\n",
    "### 6.2 KServe Î∞∞Ìè¨ ÌôïÏù∏ (ÏÑ†ÌÉùÏÇ¨Ìï≠)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KServe InferenceService ÏÉÅÌÉú ÌôïÏù∏\n",
    "!kubectl get inferenceservices -n {USER_NAMESPACE}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API ÌÖåÏä§Ìä∏ (KServe Î∞∞Ìè¨ ÏôÑÎ£å ÌõÑ)\n",
    "import json\n",
    "\n",
    "# ÌÖåÏä§Ìä∏ Îç∞Ïù¥ÌÑ∞ (8Í∞ú + ÌååÏÉù ÌîºÏ≤ò)\n",
    "test_input = {\n",
    "    \"instances\": [\n",
    "        [0.5, 0.3, 0.2, 0.1, -0.2, 0.4, 0.1, -0.5, 0.15]  # 9Í∞ú ÌîºÏ≤ò (ÌååÏÉù ÌîºÏ≤ò 1Í∞ú Ìè¨Ìï®)\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(\"ÌÖåÏä§Ìä∏ ÏûÖÎ†•:\")\n",
    "print(json.dumps(test_input, indent=2))\n",
    "print(\"\")\n",
    "print(\"curl Î™ÖÎ†πÏñ¥:\")\n",
    "print(f\"curl -X POST http://{TEAM_NAME}-model.{USER_NAMESPACE}.svc.cluster.local/v1/models/{TEAM_NAME}-model:predict \\\\\")\n",
    "print(f\"  -H 'Content-Type: application/json' \\\\\")\n",
    "print(f\"  -d '{json.dumps(test_input)}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚úÖ Ï≤¥ÌÅ¨Î¶¨Ïä§Ìä∏\n",
    "\n",
    "### ÌååÏù¥ÌîÑÎùºÏù∏ Íµ¨ÌòÑ\n",
    "- [ ] ÌåÄ ÏÑ§Ï†ï Î≥ÄÍ≤Ω ÏôÑÎ£å (TEAM_NAME)\n",
    "- [ ] Feature EngineeringÏóê ÏµúÏÜå 1Í∞ú ÌîºÏ≤ò Ï∂îÍ∞Ä\n",
    "- [ ] ÌååÏù¥ÌîÑÎùºÏù∏ Ïª¥ÌååÏùº ÏÑ±Í≥µ\n",
    "- [ ] YAML ÌååÏùº ÏÉùÏÑ± ÌôïÏù∏\n",
    "\n",
    "### ÌååÏù¥ÌîÑÎùºÏù∏ Ïã§Ìñâ\n",
    "- [ ] Kubeflow UIÏóê ÌååÏù¥ÌîÑÎùºÏù∏ ÏóÖÎ°úÎìú\n",
    "- [ ] Run ÏÉùÏÑ± Î∞è Ïã§Ìñâ\n",
    "- [ ] Î™®Îì† Ïª¥Ìè¨ÎÑåÌä∏ Succeeded ÏÉÅÌÉú\n",
    "\n",
    "### Í≤∞Í≥º ÌôïÏù∏\n",
    "- [ ] MLflowÏóêÏÑú Experiment/Run ÌôïÏù∏\n",
    "- [ ] ÌååÎùºÎØ∏ÌÑ∞/Î©îÌä∏Î¶≠ Í∏∞Î°ù ÌôïÏù∏\n",
    "- [ ] (ÏÑ†ÌÉù) KServe InferenceService Ready ÏÉÅÌÉú\n",
    "\n",
    "### Î∞úÌëú Ï§ÄÎπÑ\n",
    "- [ ] ÏãúÏó∞ ÌôîÎ©¥ Ï§ÄÎπÑ (Kubeflow UI, MLflow UI)\n",
    "- [ ] ÏΩîÎìú ÏÑ§Î™Ö Ï§ÄÎπÑ\n",
    "- [ ] Q&A ÏòàÏÉÅ ÏßàÎ¨∏ Ï§ÄÎπÑ\n",
    "\n",
    "---\n",
    "\n",
    "## üéâ ÏàòÍ≥†ÌïòÏÖ®ÏäµÎãàÎã§!\n",
    "\n",
    "Î¨∏Ï†úÍ∞Ä ÏûàÏúºÎ©¥ ÏÜêÎì§Ïñ¥ Ï£ºÏÑ∏Ïöî. Í∞ïÏÇ¨Í∞Ä ÎèÑÏôÄÎìúÎ¶¨Í≤†ÏäµÎãàÎã§.\n",
    "\n",
    "---\n",
    "\n",
    "**ÌòÑÎåÄÏò§ÌÜ†ÏóêÎ≤Ñ MLOps Training**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbformat_minor": 5,
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
